---
layout: post
title: Variational Inference
date: 2024-05-03
category: Probabilistic-Modelling
use_math: true
---

# Variational inference (VI)란 무엇인가?

[매우 잘 설명되어있는 글,,,](https://velog.io/@gibonki77/Inference-1)

## 1. 베이즈 정리와 함의, 최대사후확률추정 (MAP)
- (중학교 3학년 때부터 지금까지 베이즈 정리만 총 5번을 배웠는데 5번 다 까먹었다.)

### 베이즈 정리는 "확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리"다. 이게 대체 무슨 말임?
- **원인 사건 A1 ~ An 와 이에 따른 결과 사건 B가 주어져 있을 때, 만약 B가 발생했다면 그 원인이 사건 Ai일 확률**을 의미한다.
- 이를 수식으로 나타내면 다음과 같다.  

$$ P(A_i \mid B) = \frac{P(B \mid A_i) \times P(A_i)}{\sum\limits_{i=1}^n P(B \mid A_i) \times P(A_i)} $$

- $$ \sum\limits_{i=1}^n P(B \mid A_i) \times P(A_i) $$ 는 어떤 경우엔 $$ P(B) $$ 로 요약할 수도 있다. 이는 말 그대로 사건이 일어날 전체 확률이다.
- $$ P(A_i) $$는 $$ P(B) $$가 발생하기 전부터 알고 있던 $$ A_i $$의 발생 확률이다.
- $$ P(B \mid A_i) $$는 사건 A_i가 발생했을 때 이에 따라 사건 B가 발생할 확률이다.

### 이 베이즈 정리와 데이터 추론은 대체 무슨 상관이 있을까?
- 식을 약간 바꿔서, 원인 사건 A는 가설 h로 바꾸고 결과 사건 B는 데이터 D로 바꿔보자.

$$ P(h \mid D) = \frac{P(D \mid h) \times P(h)}{\sum\limits_{h' \in \mathbf{H}} P(D, h')}$$

- P(h)는 **사전확률 (prior)** 이다. 데이터를 관측하기 전 가지고 있던 가설을 의미한다.
- P(D\|h)는 **가능도 (likelihood)** 이다. 가설 h에 따라서 D가 발생할 확률이다.
- p(h\|D)는 **사후확률 (posterior)** 이다. 데이터 D가 관측되었을 때 그 원리, 또는 데이터의 기저에 있는 추상화된 관계가 가설 h에 기반할 확률이다.
- 마지막으로 $$ \sum\limits_{h' \in \mathbf{H}} P(D, h') $$ 는  **증거(evidence) 또는 Marginal likelihood**라고 부른다.
- 즉, 베이지언 데이터 추론은 **이미 D를 관측했을 때, 우리의 사전 지식에 기반하여 그 D의 원인이 되는 h를 추론하는 과정**이다.
    - 예를 들면, 시간 t = \[1, 2, 3\]에 따른 데이터 x = \[2, 4, 6\]이 주어졌다면 기초적인 수학 지식에 기반해 관계를 x = 2t 라고 유추해볼 수 있다.
	- 또는, x = (t-1)(t-2)(t-3) + 2t 라는 결과를 내놓을 수도 있다. (이는 일반적으로 데이터에 과적정된 추론이라고도 볼 수 있을 것이다.)
    - 답이 어느 쪽이든, 여기서 데이터 D는 주어진 t, x의 집합이고, 가설 h는 (상식적인 사전 지식 선에서 떠올릴 수 있는) 모든 t와 x 사이의 관계식이다.
- 그리고 베이즈 정리에 따른 *가장 그럴듯한 답*은 사후확률이 가장 큰 h, 즉 P(h\|D)를 최대화하는 h이다.
    - 따라서 베이즈 추론은 이런 h ($$ = \underset{h}{\mathrm{argmax}} \ P(h \mid D) $$)를 구하는 과정이라고도 볼 수 있다.

### 이때 $$ \sum\limits_{h' \in \mathbf{H}} P(D, h') $$는 처음부터 가설 공간에 대해 정해져 있는 값이다.
- 따라서 $$ \underset{h}{\mathrm{argmax}} \ P(h \mid D) $$를 구하는 과정은 자연스럽게 $$ \underset{h}{\mathrm{argmax}} \ P(D \mid h) \times P(h) $$를 구하는 과정이 된다.
- 이러한 추론을 **최대사후확률추정 (Maximum A Posteriori, MAP)** 라고 한다.



## 2. 최대가능도추정 (MLE)
- 한편 MAP와 자주 비교되는 MLE란 무엇인가?
    - 이것 역시 학부 확률과통계, 기초확률론 등 필수 이수과목에서 분명 2번이나 가르쳤던 것 같은데 또 까먹었다.

### 최대가능도추정 (Maximum Likelihood Estimation, MLE) 역시 데이터에 기반해 모수를 추론하는 과정이다.
- 모집단에서 임의로 추출한 것을 우리는 표본(sample)이라 한다. MLE는 거꾸로 표본으로부터 모집단을 복원하는 과정이다.
- 이 때, MAP와는 달리 **사전지식이 들어가지 않고 오직 데이터만** 사용한다.
- 우리가 복원하고자 하는 모수 (parameter) $$ \theta_{MLE} $$ 는 데이터로부터 복원하고자 하는 모집단의 성질에 따라 달라진다.
    - 예를 들어 평균 $$ \mu $$와 표준편차 $$ \sigma $$를 모르는 정규분포를 그 추출값으로부터 복원하고자 할 때, 우리가 구해야 할 parameter는 $$ \theta = (\mu, \sigma) $$ 가 된다.
- 이때 뭔진 모르겠지만 sample 및 parameter에 대한 가능도(likelihood)를 최대화함으로써 parameter를 복원하기 때문에 MLE라고 부른다.

### 가능도는 그러면 어떻게 구하는가?
- 가능도 $$ L(\theta ; x) $$은 sample x가 이미 정해진 값일 때 parameter θ에 따라 달라지는 함수다.
	- 이때 식 자체는 원래 parameter에 따른 확률밀도함수인 $$ p(x ; \theta) $$와 같다. 그저 무엇이 상수이고 무엇이 변수인지가 달라졌을 뿐이다.
- 이게 무슨 말이냐면...
	- 이를테면 다음과 같은 정규분포 확률밀도함수가 있다고 치자.
	
	$$ p(x ; \mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^2}}exp(-\frac{(x - \mu)^2}{2\sigma^2}) $$
	
	- 위 식을 x에 대한 확률분포로 본다면, 어떤 변수 x의 값을 넣었을 때 그 x와 주어진 평균, 표준편차에 대한 확률밀도 값이 튀어나올 것이다.
	- Likelihood function은 반대로, x는 고정되어 있다. 그리고 우리는 $$ \mu, \sigma $$의 값을 변화시키면서 가능도 함수가 어떻게 바뀌는지 확인할 수 있다.
	
	$$ L(\mu, \sigma ; x) = \frac{1}{\sqrt{2 \pi \sigma^2}}exp(-\frac{(x - \mu)^2}{2\sigma^2}) $$
	
	- 만약 parameter $$ \mu, \sigma $$를 제대로 찾았다면, $$ L(\mu, \sigma) $$는 커진다. 그래서 이것을 가능도(likelihood)라고 부른다. 이거 가능해보임? ㅇㅇ ㄱㄴ

## 3. MLE와 MAP